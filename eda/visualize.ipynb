{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of hdf5 VR demos.\n",
    "\n",
    "Source: https://github.com/StanfordVL/behavior/blob/main/docs/vr_demos.md\n",
    "\n",
    "## Processed demos\n",
    "The following are the available keys to index into the hdf5 file. The dimensionality of each component is noted parenthetically, where `N` indicates the number of frames in the demo.\n",
    "\n",
    "- action (N x 28) -- see BehaviorRobot description in the [Embodiments section](agents.md) for details about the actuation of this robot. This vector contains two additional dimensions that correspond to the `hand reset` action in VR: an action that teleports the simulated hands to the exact pose of the VR hand controller when they have diverged. These actions are not used by AI agents but are necessary to understand the demos.\n",
    "- proprioception (N x 22) -- proprioceptive feedback. More details in the [Embodiments section](agents.md).\n",
    "- rgb (N x 128 x 128 x 3) -- rgb image from camera\n",
    "- depth (N x 128 x 128 x 1) -- depth map\n",
    "- seg (N x 128 x 128 x 1) -- segmentation of scene\n",
    "- ins_seg (N x 128 x 128 x 1) -- instance segmentation\n",
    "- highlight ( N x 128 x 128 x 1) -- activity relevant object binary mask, active for all objects included in the activity goal (except the agent and the floor)\n",
    "- task_obs (N x 456) -- task observations, including ground truth state of the robot, and ground truth poses and grasping state of a maximum of a fixed number of activity relevant objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 File Structure:\n",
      "- action\n",
      "  (Dataset: shape=(3000, 28), dtype=float64)\n",
      "- depth\n",
      "  (Dataset: shape=(3000, 128, 128, 1), dtype=float32)\n",
      "- highlight\n",
      "  (Dataset: shape=(3000, 128, 128, 1), dtype=float32)\n",
      "- ins_seg\n",
      "  (Dataset: shape=(3000, 128, 128, 1), dtype=float32)\n",
      "- proprioception\n",
      "  (Dataset: shape=(3000, 22), dtype=float32)\n",
      "- rgb\n",
      "  (Dataset: shape=(3000, 128, 128, 3), dtype=float32)\n",
      "- seg\n",
      "  (Dataset: shape=(3000, 128, 128, 1), dtype=float32)\n",
      "- task_obs\n",
      "  (Dataset: shape=(3000, 456), dtype=float32)\n",
      "<KeysViewHDF5 ['/metadata/activity', '/metadata/activity_id', '/metadata/physics_timestep', '/metadata/render_timestep', '/metadata/scene_id', '/metadata/vr_settings']>\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "hdf5_path=\"/datasets/behavior-100-replay/bottling_fruit_0_Wainscott_0_int_0_2021-05-24_19-46-46_episode.hdf5\"\n",
    "\n",
    "\n",
    "def print_hdf5_tree(file, indent=0):\n",
    "    \"\"\"\n",
    "    Recursively prints the structure of an HDF5 file in tree form.\n",
    "    \"\"\"\n",
    "    if isinstance(file, h5py.Group):  # If it's a group, iterate through its items\n",
    "        for key, item in file.items():\n",
    "            print(\"  \" * indent + f\"- {key}\")\n",
    "            if isinstance(item, h5py.Group):\n",
    "                print_hdf5_tree(item, indent + 1)  # Recurse into groups\n",
    "            elif isinstance(item, h5py.Dataset):\n",
    "                print(\"  \" * (indent + 1) + f\"(Dataset: shape={item.shape}, dtype={item.dtype})\")\n",
    "\n",
    "\n",
    "with h5py.File(hdf5_path, 'r') as hdf_file:\n",
    "    print(\"HDF5 File Structure:\")\n",
    "    print_hdf5_tree(hdf_file)\n",
    "    print(hdf_file.attrs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with h5py.File(hdf5_path, \"r\") as hdf_file:\n",
    "    depth = hdf_file[\"depth\"][:]          # (3000, 128, 128, 1)\n",
    "    rgb = hdf_file[\"rgb\"][:]              # (3000, 128, 128, 3)\n",
    "    seg = hdf_file[\"seg\"][:]              # (3000, 128, 128, 1)\n",
    "    highlight = hdf_file[\"highlight\"][:]  # (3000, 128, 128, 1)\n",
    "    \n",
    "np.unique(highlight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to visualizations.mp4\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import av\n",
    "import cv2\n",
    "\n",
    "\n",
    "# Open the HDF5 file\n",
    "\n",
    "with h5py.File(hdf5_path, \"r\") as hdf_file:\n",
    "    depth = hdf_file[\"depth\"][:]          # (3000, 128, 128, 1)\n",
    "    rgb = hdf_file[\"rgb\"][:]              # (3000, 128, 128, 3)\n",
    "    seg = hdf_file[\"seg\"][:]              # (3000, 128, 128, 1)\n",
    "    highlight = hdf_file[\"highlight\"][:]  # (3000, 128, 128, 1)\n",
    "\n",
    "# Normalize depth and highlight to [0, 255] and convert to uint8\n",
    "depth = (depth.squeeze() * 255).clip(0, 255).astype(np.uint8)\n",
    "highlight = (highlight.squeeze() * 255).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "# Convert RGB from [0, 1] to [0, 255] and uint8\n",
    "rgb = (rgb * 255).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "# Segmented RGB (RGB + Segmentation overlay)\n",
    "seg_colored = np.stack([seg.squeeze()] * 3, axis=-1) * 255  # Convert seg to 3-channel mask\n",
    "segmented_rgb = (rgb + seg_colored).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "# RGB with highlight overlay\n",
    "highlight_colored = np.stack([highlight] * 3, axis=-1)      # Convert highlight to 3-channel mask\n",
    "rgb_highlight = (rgb + highlight_colored).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "# Output video path\n",
    "output_path = \"visualizations.mp4\"\n",
    "fps = 30\n",
    "\n",
    "# Create a PyAV video container\n",
    "container = av.open(output_path, mode='w')\n",
    "stream = container.add_stream(\"h264\", rate=fps)\n",
    "# stream = container.add_stream(\"mpeg4\", rate=fps)\n",
    "stream.width = 128 * 4  # Combined frame width\n",
    "stream.height = 128     # Frame height\n",
    "stream.pix_fmt = \"yuv420p\"\n",
    "\n",
    "# Create video frames\n",
    "for i in range(depth.shape[0]):\n",
    "    # Prepare each frame\n",
    "    depth_frame = cv2.cvtColor(depth[i], cv2.COLOR_GRAY2BGR)  # Convert to 3 channels\n",
    "    rgb_frame = rgb[i]\n",
    "    seg_rgb_frame = segmented_rgb[i]\n",
    "    highlight_frame = rgb_highlight[i]\n",
    "\n",
    "    # Concatenate frames horizontally\n",
    "    combined_frame = np.hstack((depth_frame, rgb_frame, seg_rgb_frame, highlight_frame))\n",
    "    \n",
    "    # Convert to PyAV format (RGB to YUV420)\n",
    "    frame = av.VideoFrame.from_ndarray(combined_frame, format=\"rgb24\")\n",
    "    packet = stream.encode(frame)\n",
    "    if packet:\n",
    "        container.mux(packet)\n",
    "\n",
    "# Finalize the video\n",
    "container.close()\n",
    "print(f\"Video saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"visualizations.mp4\" type=\"video/mp4\">\n",
       "  Your browser does not support the video tag.\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "from IPython.display import Video\n",
    "\n",
    "# Path to the video file\n",
    "video_path = \"visualizations.mp4\"\n",
    "\n",
    "# Display the video in the notebook\n",
    "HTML(f\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{video_path}\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw VR demos\n",
    "The metadata can be accessed by keying into the hdf5.attrs with the following keys:\n",
    "- `/metadata/start_time`: the date the demo was recorded\n",
    "- `/metadata/physics_timestep`: the simulated time duration of each step of the physics simulator (1/300 seconds for all our demos)\n",
    "- `/metadata/render_timestep`: the simulated time between each rendered image, determines the framerate (1/30 seconds). `render_timestep / physics_timestep` gives the number of physics simulation steps between two generated images (10)\n",
    "- `/metadata/git_info`: the git info for activity definition, `iGibson`, `ig_dataset`, and `ig_assets`. This is used to ensure participants are using a compatible version of iGibson if replaying the demo\n",
    "- `/metadata/task_name`: The name of the activity, e.g. `washing_dishes`, `putting_away_groceries`...\n",
    "- `/metadata/task_instance`: The instance of the activity that specifies the state (pose, extended state) of the sampled activity relevant objects at initialization\n",
    "- `/metadata/scene_id`: The scene (`Rs_int`, `Wainscott_0_int`, etc.) where the activity was recorded\n",
    "- `/metadata/filter_objects`: Whether only activity relevant objects were recorded in the activity\n",
    "- `/metadata/obj_body_id_to_name`: mapping of pybullet IDs to the semantic object name\n",
    "\n",
    "\n",
    "### HDF5 Content\n",
    "\n",
    "The following are the available keys to index into the hdf5 file. The dimensionality of each component is noted parenthetically, where `N` indicates the number of frames in the demo.\n",
    "\n",
    "- `frame_data` (N x 4) \n",
    "    - `goal_status` \n",
    "        - `satisfied` (N x total_goals) -- Total satisfied top-level predicates, where total_goals is the number of predicates \n",
    "        - `unsatisfied` (N x total_goals) -- Total unsatisfied top-level predicates, where total_goals is the number of predicates \n",
    "- `physics_data` \n",
    "    - `string` (bullet_id: total number of activity-relevant scene objects)\n",
    "    - `position` (N x 3) -- The 3D position of the object center of mass\n",
    "    - `orientation` (N x 4) -- The quaternion orientation of the object\n",
    "    - `joint_state` (N x number of object joints) -- The pybullet joint state of each object \n",
    "- `vr` \n",
    "    - `vr_camera`\n",
    "        - `right_eye_view` (N x 4 x 4) -- the view projection matrix\n",
    "        - `right_eye_proj` (N x 4 x 4) -- the camera projection matrix \n",
    "        - `right_camera_pos` (N x 3) -- The 3D position of the camera \n",
    "    - `vr_device_data` \n",
    "        - `hmd` (N x 17) -- see below\n",
    "        - `left_controller` (N x 27) -- see below\n",
    "        - `right_controller` (N x 27) -- see below \n",
    "        - `vr_position_data` (N x 12) -- see below\n",
    "        - `torso_tracker` (N x 8) -- see below\n",
    "    - `vr_button_data`\n",
    "        - `left_controller` (N x 3) -- see below\n",
    "        - `right_controller` (N x 3) -- see below\n",
    "    - `vr_eye_tracking_data` \n",
    "        - `left_controller` (N x 9) -- see below \n",
    "    - `vr_event_data` \n",
    "        - `left_controller` (N x 28) -- see below\n",
    "        - `right_controller` (N x 28) -- see below\n",
    "        - `reset_actions` (N x 2) -- reset for left and right controller\n",
    "- `Agent_actions`\n",
    "    - `vr_robot` (N x 28) -- see BEHAVIOR robot description in previous section\n",
    "- `action` -- unused \n",
    "\n",
    "Additional description of the dimensions of the arrays noted above: the following are not keys but correspond to indices of the associated array:\n",
    "\n",
    "- `hmd` (17) \n",
    "    - hmd tracking data is valid (1)\n",
    "    - translation (3) \n",
    "    - rotation (4)\n",
    "    - right vector (3) \n",
    "    - up vector (3)\n",
    "    - forward vector (3)\n",
    "- `left_controller`/`right_controller` (27) \n",
    "    - controller tracking data is valid (1)\n",
    "    - translation (3)\n",
    "    - rotation (4)\n",
    "    - right vector (3) \n",
    "    - up vector (3)\n",
    "    - forward vector (3)\n",
    "    - base_rotation (4)\n",
    "    - base_rotation * controller_rotation (4)\n",
    "    - applied_force (6) -- p.getConstraintState(controller_constraint_id)\n",
    "- `vr_button_data`\n",
    "    - trigger fraction (1) --- open: 0 -> closed: 1 \n",
    "    - touchpad x position (1) -- left: -1 -> right: 1\n",
    "    - touchpad y position (1) -- bottom: -1 -> right: 1\n",
    "- `Vr_eye_tracking_data`\n",
    "    - eye-tracking data is valid (1)\n",
    "    - origin of gaze in world space (3) \n",
    "    - direction vector of gaze in world space (3)\n",
    "    - left pupil diameter (1)\n",
    "    - right pupil diameter (1) \n",
    "- `vr_position_data` (12)\n",
    "    - position of the system in iGibson space (3)\n",
    "    - offset of the system from the origin (3)\n",
    "    - applied force to vr body (6) -- p.getConstraintState(body_constraint_id)\n",
    "- `torso_tracker` (8)\n",
    "    - torso tracker is valid (1)\n",
    "    - position (3)\n",
    "    - rotation (4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 File Structure:\n",
      "- agent_actions\n",
      "  - vr_robot\n",
      "    (Dataset: shape=(3000, 28), dtype=float64)\n",
      "- frame_data\n",
      "  (Dataset: shape=(3000, 4), dtype=float64)\n",
      "- goal_status\n",
      "  - satisfied\n",
      "    (Dataset: shape=(3000, 5), dtype=float64)\n",
      "  - unsatisfied\n",
      "    (Dataset: shape=(3000, 5), dtype=float64)\n",
      "- physics_data\n",
      "  - 1\n",
      "    - joint_state\n",
      "      (Dataset: shape=(3000, 0), dtype=float64)\n",
      "    - orientation\n",
      "      (Dataset: shape=(3000, 4), dtype=float64)\n",
      "    - position\n",
      "      (Dataset: shape=(3000, 3), dtype=float64)\n",
      "  - 103\n",
      "    - joint_state\n",
      "      (Dataset: shape=(3000, 3), dtype=float64)\n",
      "    - orientation\n",
      "      (Dataset: shape=(3000, 4), dtype=float64)\n",
      "    - position\n",
      "      (Dataset: shape=(3000, 3), dtype=float64)\n",
      "  - 107\n",
      "    - joint_state\n",
      "      (Dataset: shape=(3000, 0), dtype=float64)\n",
      "    - orientation\n",
      "      (Dataset: shape=(3000, 4), dtype=float64)\n",
      "    - position\n",
      "      (Dataset: shape=(3000, 3), dtype=float64)\n",
      "  - 120\n",
      "    - joint_state\n",
      "      (Dataset: shape=(3000, 1), dtype=float64)\n",
      "    - orientation\n",
      "      (Dataset: shape=(3000, 4), dtype=float64)\n",
      "    - position\n",
      "      (Dataset: shape=(3000, 3), dtype=float64)\n",
      "  - 163\n",
      "    - joint_state\n",
      "      (Dataset: shape=(3000, 1), dtype=float64)\n",
      "    - orientation\n",
      "      (Dataset: shape=(3000, 4), dtype=float64)\n",
      "    - position\n",
      "      (Dataset: shape=(3000, 3), dtype=float64)\n",
      "  - 164\n",
      "    - joint_state\n",
      "      (Dataset: shape=(3000, 1), dtype=float64)\n",
      "    - orientation\n",
      "      (Dataset: shape=(3000, 4), dtype=float64)\n",
      "    - position\n",
      "      (Dataset: shape=(3000, 3), dtype=float64)\n",
      "  - 165\n",
      "    - joint_state\n",
      "      (Dataset: shape=(3000, 1), dtype=float64)\n",
      "    - orientation\n",
      "      (Dataset: shape=(3000, 4), dtype=float64)\n",
      "    - position\n",
      "      (Dataset: shape=(3000, 3), dtype=float64)\n",
      "  - 199\n",
      "    - joint_state\n",
      "      (Dataset: shape=(3000, 0), dtype=float64)\n",
      "    - orientation\n",
      "      (Dataset: shape=(3000, 4), dtype=float64)\n",
      "    - position\n",
      "      (Dataset: shape=(3000, 3), dtype=float64)\n",
      "  - 202\n",
      "    - joint_state\n",
      "      (Dataset: shape=(3000, 0), dtype=float64)\n",
      "    - orientation\n",
      "      (Dataset: shape=(3000, 4), dtype=float64)\n",
      "    - position\n",
      "      (Dataset: shape=(3000, 3), dtype=float64)\n",
      "  - 270\n",
      "    - joint_state\n",
      "      (Dataset: shape=(3000, 1), dtype=float64)\n",
      "    - orientation\n",
      "      (Dataset: shape=(3000, 4), dtype=float64)\n",
      "    - position\n",
      "      (Dataset: shape=(3000, 3), dtype=float64)\n",
      "- vr\n",
      "  - vr_button_data\n",
      "    - left_controller\n",
      "      (Dataset: shape=(3000, 3), dtype=float64)\n",
      "    - right_controller\n",
      "      (Dataset: shape=(3000, 3), dtype=float64)\n",
      "  - vr_camera\n",
      "    - right_camera_pos\n",
      "      (Dataset: shape=(3000, 3), dtype=float64)\n",
      "    - right_eye_proj\n",
      "      (Dataset: shape=(3000, 4, 4), dtype=float64)\n",
      "    - right_eye_view\n",
      "      (Dataset: shape=(3000, 4, 4), dtype=float64)\n",
      "  - vr_device_data\n",
      "    - hmd\n",
      "      (Dataset: shape=(3000, 17), dtype=float64)\n",
      "    - left_controller\n",
      "      (Dataset: shape=(3000, 27), dtype=float64)\n",
      "    - right_controller\n",
      "      (Dataset: shape=(3000, 27), dtype=float64)\n",
      "    - torso_tracker\n",
      "      (Dataset: shape=(3000, 8), dtype=float64)\n",
      "    - vr_position_data\n",
      "      (Dataset: shape=(3000, 12), dtype=float64)\n",
      "  - vr_event_data\n",
      "    - left_controller\n",
      "      (Dataset: shape=(3000, 28), dtype=float64)\n",
      "    - reset_actions\n",
      "      (Dataset: shape=(3000, 2), dtype=float64)\n",
      "    - right_controller\n",
      "      (Dataset: shape=(3000, 28), dtype=float64)\n",
      "  - vr_eye_tracking_data\n",
      "    (Dataset: shape=(3000, 9), dtype=float64)\n",
      "<KeysViewHDF5 ['/metadata/activity_definition', '/metadata/atus_activity', '/metadata/filter_objects', '/metadata/git_info', '/metadata/instance_id', '/metadata/obj_body_id_to_name', '/metadata/physics_timestep', '/metadata/render_timestep', '/metadata/scene_id', '/metadata/start_time', '/metadata/urdf_file', '/metadata/vr_settings']>\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "hdf5_path=\"/datasets/behavior-100-replay/bottling_fruit_0_Wainscott_0_int_0_2021-05-24_19-46-46.hdf5\"\n",
    "\n",
    "\n",
    "def print_hdf5_tree(file, indent=0):\n",
    "    \"\"\"\n",
    "    Recursively prints the structure of an HDF5 file in tree form.\n",
    "    \"\"\"\n",
    "    if isinstance(file, h5py.Group):  # If it's a group, iterate through its items\n",
    "        for key, item in file.items():\n",
    "            print(\"  \" * indent + f\"- {key}\")\n",
    "            if isinstance(item, h5py.Group):\n",
    "                print_hdf5_tree(item, indent + 1)  # Recurse into groups\n",
    "            elif isinstance(item, h5py.Dataset):\n",
    "                print(\"  \" * (indent + 1) + f\"(Dataset: shape={item.shape}, dtype={item.dtype})\")\n",
    "\n",
    "\n",
    "with h5py.File(hdf5_path, 'r') as hdf_file:\n",
    "    print(\"HDF5 File Structure:\")\n",
    "    print_hdf5_tree(hdf_file)\n",
    "    print(hdf_file.attrs.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
